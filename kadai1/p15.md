---
layout: page
title: 1-5. Luhnアルゴリズム
---

クレジットカード番号に誤りがないか確認するため，[Luhnアルゴリズム](https://ja.wikipedia.org/wiki/Luhn%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0)が用いられている．このアルゴリズムは，入力された番号が「正しい」か「間違っている」かを，以下の手順で求める．

1. 右端の数字を1番目として，偶数番目の数字を2倍する
2. 1.の計算の後，すべての数字の総和を求める．ただし，1.の処理で数字が2桁になった場合は，1桁目と2桁目の数字を足す．
3. 求めた総和の1の位が0であれば「正しい」，そうでなければ「間違っている」と判定する

例えば，49927398716という番号が入力されたとき，

![Luhnアルゴリズムの動作例](img/luhn.png)

より$70$が求まるため，このカード番号は「正しい」と判定する．

100桁までのカード番号を標準入力から読み取り，Luhnアルゴリズムで正当性を検証し，正しければ`true`，間違っていれば`false`と表示するプログラムを作成せよ．

## 実行例
`#`は標準入力，`>`は標準出力を表す．

```
# 49927398716
> true
```

```
# 49927398714
> false
```

```
# 1982049058395830128494
> true
```

## 実装のポイント

入力されるカード番号の桁数は最大で100桁なので，整数型（例えば`unsigned long long`型）ではカード番号を正しく保持できない恐れがある．
また，整数型でカード番号を読み取ってしまうと，各桁の数字を取り出す処理が煩雑になる（10で割ったり，10で割った余りを求める処理を繰り返すことになる）．
そこで，カード番号を文字（数字）の配列，すなわち文字列として読み込むことを考える．

+ 文字列（文字の配列）
+ ASCIIコード
